# Configuration for GRPO with MiniLLM-style Reward (Teacher LL + L_PT)

# Training parameters
training:
  epochs: 3
  max_steps: 20000
  save_interval: 500
  eval_interval: 100
  log_interval: 10
  seed: 42
  
  # Optimization
  learning_rate: 5e-6
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  weight_decay: 0.01
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  
  # Mixed precision
  fp16: true
  bf16: false
  
  # Checkpointing
  resume_from_checkpoint: null
  save_total_limit: 3
  
# GRPO specific parameters
grpo:
  group_size: 4
  kl_weight: 0.1
  lpt_weight: 0.1  # Weight for L_PT component (Î»)
  epsilon: 1e-8
  reference_model: initial  # Options: initial, previous_step, fixed

# Model configuration
model:
  # Student model
  student:
    name: mistral-7b-v0.1
    model_type: causal_lm
    load_in_8bit: false
    gradient_checkpointing: true
    
  # Teacher model
  teacher:
    name: gemma-27b
    model_type: causal_lm
    load_in_8bit: true
    device: cuda
    
  # Tokenizer
  tokenizer:
    add_faroese_tokens: true
    max_length: 2048
    padding_side: right

# Reward configuration
reward:
  # Teacher log-probability component
  teacher_logprob:
    normalize: true
    sequence_level: true
    temperature: 1.0
  
  # Language modeling loss component (L_PT)
  lpt:
    enabled: true
    weight: 0.1
    faroese_specific: true
    faroese_char_factor: 1.2

# Data configuration
data:
  # Data for response generation (prompt-response pairs)
  train_file: data/faroese/train.jsonl
  eval_file: data/faroese/eval.jsonl
  prompt_column: prompt
  response_column: response
  
  # Data for language modeling (L_PT)
  pretraining_file: data/faroese/corpus.txt
  
  # Batch sizes
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  pretraining_batch_size: 4
  
  # Preprocessing
  preprocessing:
    use_faroese_preprocessing: true
    max_length: 2048
    
# Evaluation
evaluation:
  metrics:
    - foqa
    - perplexity
    - bleu
  generation_config:
    max_new_tokens: 256
    do_sample: true
    temperature: 0.7
    top_p: 0.9
    
# Logging
logging:
  report_to: ["tensorboard", "wandb"]
  wandb_project: grpo-faroese-kd
  wandb_run_name: grpo_minillm_run 