# GRPO-KD: Group Relative Policy Optimization with Knowledge Distillation for Faroese Language Models

## Project Overview

This research project focuses on enhancing smaller language models for the Faroese language using Group Relative Policy Optimization (GRPO) combined with Knowledge Distillation (KD) techniques. The goal is to create more efficient and effective language models while preserving language-specific capabilities.

## Key Features

- Implementation of GRPO with Knowledge Distillation rewards
- Support for multiple student and teacher model architectures
- Specialized training for Faroese language understanding
- Comprehensive evaluation on Faroese language benchmarks
- HPC/SLURM integration for distributed training

## Project Structure

```
grpo-kd-research/
├── data/               # Datasets and benchmarks
├── src/               # Source code implementation
├── configs/           # Configuration files
├── scripts/           # Utility and SLURM scripts
├── experiments/       # Experiment tracking
├── results/           # Results and model checkpoints
├── notebooks/         # Analysis notebooks
└── docs/             # Documentation
```

## Getting Started

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- Transformers library
- SLURM environment for distributed training

### Installation

```bash
# Clone the repository
git clone [repository-url]
cd grpo-kd-research

# Install dependencies (to be added)
pip install -r requirements.txt
```

## Usage

Detailed usage instructions will be added as the project develops.

## License

This project is part of academic research at [Institution Name].

## Acknowledgments

- Research supported by [funding/institution details]
- Based on GRPO and MiniLLM methodologies 