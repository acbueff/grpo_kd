{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing with TensorFlow\n",
    "In this notebook we will go through checkpointing your model with TensorFlow.\n",
    "\n",
    "## Setting up model and dataset\n",
    "For this example we will use [Tiny ImageNet](https://www.kaggle.com/c/tiny-imagenet/overview) which is similar to ImageNet but lower resolution (64x64) and fewer images (100 k). For this dataset we will use a variant of the ResNet architecture which is a type of Convolutional Neural Network with residual connections. For the sake of this tutorial you do not need to understand the details about the model or the dataset. But you can read up more about the dataloading information in task `3_loading_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, layers, Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(layers.Layer):\n",
    "    \n",
    "    def __init__(self, filters, strides=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.filters = filters\n",
    "        self.strides = strides\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        self.relu = layers.ReLU(name='relu')\n",
    "        self.conv1 = layers.Conv2D(filters, 3, strides=strides, padding=\"same\", use_bias=False, name='conv1')\n",
    "        self.bn1 = layers.BatchNormalization(epsilon=1e-5, name='bn1')\n",
    "        self.conv2 = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False, name='conv2')\n",
    "        self.bn2 = layers.BatchNormalization(epsilon=1e-5, name='bn2')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        prev_shape = x.shape\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        identity = inputs if self.downsample is None else self.downsample(inputs)\n",
    "    \n",
    "        return self.relu(x + identity)\n",
    "\n",
    "\n",
    "class ResNet(keras.Model):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers,\n",
    "        num_classes=1000,\n",
    "        zero_init_residual=False,\n",
    "        groups=1,\n",
    "        downsample=None,\n",
    "        name=\"resnet\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.block = ResidualBlock\n",
    "        \n",
    "        self.in_filters = 64\n",
    "        self.dilation = 1\n",
    "        self.groups = 1\n",
    "        \n",
    "        # Defining layers\n",
    "        self.relu = layers.ReLU(name='relu')\n",
    "        self.conv1 = layers.Conv2D(filters=64, kernel_size=7, strides=2, padding=\"same\", use_bias=False, name='conv1')\n",
    "        self.bn1 = layers.BatchNormalization(epsilon=1e-5, name='bn1')\n",
    "        self.maxpool = layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\", name='maxpool')\n",
    "        self.layer1 = self._make_layer(64, n_layers[0], name='layer1')\n",
    "        self.layer2 = self._make_layer(128, n_layers[1], strides=2, name='layer2')\n",
    "        self.layer3 = self._make_layer(256, n_layers[2], strides=2, name='layer3')\n",
    "        self.layer4 = self._make_layer(512, n_layers[3], strides=2, name='layer4')\n",
    "        self.avgpool = layers.AveragePooling2D(pool_size=1, name='avgpool')\n",
    "        self.flatten = layers.Flatten(name='flatten')\n",
    "        self.fc = layers.Dense(num_classes, name='fc')\n",
    "    \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, layers.Conv2D):\n",
    "                layer.kernel_initializer = keras.initializers.VarianceScaling(\n",
    "                    scale=2.0,\n",
    "                    mode=\"fan_out\",\n",
    "                )        \n",
    "    \n",
    "    \n",
    "    def _make_layer(self, filters, n_blocks, strides=1, **kwargs):\n",
    "        block = self.block\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        check_singular_strides = lambda strides: (tuple(strides) != (1, 1) if isinstance(strides, Iterable) else strides != 1)\n",
    "        if check_singular_strides(strides) or self.in_filters != filters:\n",
    "            downsample = keras.Sequential(\n",
    "                [\n",
    "                    layers.Conv2D(filters, 1, strides=strides, use_bias=False),\n",
    "                    layers.BatchNormalization(epsilon=1e-5),\n",
    "                ],\n",
    "            )\n",
    "        \n",
    "        layer = keras.Sequential(**kwargs)\n",
    "        layer.add(block(filters, strides=strides, downsample=downsample))\n",
    "        self.in_filters = filters\n",
    "        for _ in range(1, n_blocks):\n",
    "            layer.add(block(filters))\n",
    "    \n",
    "        return layer\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = ResNet([2, 2, 2, 2], num_classes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_dataset import (\n",
    "    tiny_imagenet_generator,\n",
    "    tiny_imagenet_signature,\n",
    "    tiny_imagenet_train_size,\n",
    "    tiny_imagenet_val_size,\n",
    ")\n",
    "\n",
    "\n",
    "n_epochs = 1\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    generator=partial(tiny_imagenet_generator, split='train', shuffle=True),\n",
    "    output_signature=tiny_imagenet_signature,\n",
    ").repeat(n_epochs).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    generator=partial(tiny_imagenet_generator, split='val', shuffle=True),\n",
    "    output_signature=tiny_imagenet_signature,\n",
    ").repeat(n_epochs).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with checkpoints\n",
    "Now we come to the important part, the training. In this part we will have to include the checkpointing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointing is done via callback\n",
    "checkpoint_path = \"checkpoints-tf/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,  # will not save entire model\n",
    "    mode='auto',\n",
    "    save_freq='epoch',\n",
    "    options=None,\n",
    ")\n",
    "\n",
    "# Compile model as usual\n",
    "resnet18.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to add checkpoint callback\n",
    "resnet18.fit(\n",
    "    train_dataset,\n",
    "    epochs=n_epochs,\n",
    "    steps_per_epoch=(tiny_imagenet_train_size // batch_size),\n",
    "    callbacks=[checkpoint_callback],\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=(tiny_imagenet_val_size // batch_size),\n",
    "    verbose=1,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the above run (using 8 epochs) that we get the expected single checkpoint per epoch.\n",
    "\n",
    "(As a side note, the results can be significantly improved if trained from a pretrained ResNet that is available from torchvision, but converting weights from PyTorch is a bit out of scope for this tutorial.)\n",
    "\n",
    "In this example we decided to only save weight during checkpointing but we can also save the entire model. Here we do it with the trained model in the SavedModel format (instead of hdf5 which is the other alternative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.save(\"model-tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the different directory structures of checkpointing and saving the model separetely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tree checkpoints-tf\n",
    "\n",
    "tree model-tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that in addition to the saved models we also get meta data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from checkpoint\n",
    "Now that we have created a checkpointed we want to load it to and I've also added a check to see that the loading went as planned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_model = ResNet([2, 2, 2, 2], num_classes=200)\n",
    "ckpt_model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "latest_ckpt = tf.train.latest_checkpoint(\"checkpoints-tf\")\n",
    "ckpt_model.load_weights(latest_ckpt).expect_partial()\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('model-tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_dataset:\n",
    "    y_saved  = resnet18(x)\n",
    "    y_ckpt   = ckpt_model(x)\n",
    "    y_loaded = loaded_model(x)\n",
    "    \n",
    "    # Check that models are reproduced (atleast w.r.t. relative tolerance)\n",
    "    tf.debugging.assert_near(y_ckpt, y_loaded, atol=1e-3)\n",
    "    tf.debugging.assert_near(y_saved, y_ckpt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "1. Create a cell below that continues training from the latest checkpoint\n",
    "2. Modify the training to only save the best model so far"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
