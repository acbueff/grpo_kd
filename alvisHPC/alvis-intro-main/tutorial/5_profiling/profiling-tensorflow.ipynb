{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling with TensorFlow\n",
    "In this notebook we will go through Profiling your training with TensorFlow and TensorBoard.\n",
    "\n",
    "## Setting up model and dataset\n",
    "For this example we will use [Tiny ImageNet](https://www.kaggle.com/c/tiny-imagenet/overview) which is similar to ImageNet but lower resolution (64x64) and fewer images (100 k). For this dataset we will use a variant of the ResNet architecture which is a type of Convolutional Neural Network with residual connections. For the sake of this tutorial you do not need to understand the details about the model or the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, layers, Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(layers.Layer):\n",
    "    \n",
    "    def __init__(self, filters, strides=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        self.relu = layers.ReLU()\n",
    "        self.conv1 = layers.Conv2D(filters, 3, strides=strides, padding=\"same\", use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization(epsilon=1e-5)\n",
    "        self.conv2 = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization(epsilon=1e-5)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        prev_shape = x.shape\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        identity = inputs if self.downsample is None else self.downsample(inputs)\n",
    "    \n",
    "        return self.relu(x + identity)\n",
    "\n",
    "\n",
    "class ResNet(keras.Model):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers,\n",
    "        num_classes=1000,\n",
    "        zero_init_residual=False,\n",
    "        groups=1,\n",
    "        downsample=None,\n",
    "        name=\"resnet\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.block = ResidualBlock\n",
    "        \n",
    "        self.in_filters = 64\n",
    "        self.dilation = 1\n",
    "        self.groups = 1\n",
    "        \n",
    "        # Defining layers\n",
    "        self.relu = layers.ReLU()\n",
    "        self.conv1 = layers.Conv2D(filters=64, kernel_size=7, strides=2, padding=\"same\", use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization(epsilon=1e-5)\n",
    "        self.maxpool = layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\")\n",
    "        self.layer1 = self._make_layer(64, n_layers[0])\n",
    "        self.layer2 = self._make_layer(128, n_layers[1], strides=2)\n",
    "        self.layer3 = self._make_layer(256, n_layers[2], strides=2)\n",
    "        self.layer4 = self._make_layer(512, n_layers[3], strides=2)\n",
    "        self.avgpool = layers.AveragePooling2D(pool_size=1)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(num_classes)\n",
    "    \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, layers.Conv2D):\n",
    "                layer.kernel_initializer = keras.initializers.VarianceScaling(\n",
    "                    scale=2.0,\n",
    "                    mode=\"fan_out\",\n",
    "                )        \n",
    "    \n",
    "    \n",
    "    def _make_layer(self, filters, n_blocks, strides=1):\n",
    "        block = self.block\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        check_singular_strides = lambda strides: (tuple(strides) != (1, 1) if isinstance(strides, Iterable) else strides != 1)\n",
    "        if check_singular_strides(strides) or self.in_filters != filters:\n",
    "            downsample = keras.Sequential([\n",
    "                layers.Conv2D(filters, 1, strides=strides, use_bias=False),\n",
    "                layers.BatchNormalization(epsilon=1e-5),\n",
    "            ])\n",
    "        \n",
    "        layer = keras.Sequential()\n",
    "        layer.add(block(filters, strides=strides, downsample=downsample))\n",
    "        self.in_filters = filters\n",
    "        for _ in range(1, n_blocks):\n",
    "            layer.add(block(filters))\n",
    "    \n",
    "        return layer\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = ResNet([2, 2, 2, 2], num_classes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing datasets\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_dataset import (\n",
    "    tiny_imagenet_generator,\n",
    "    tiny_imagenet_signature,\n",
    "    tiny_imagenet_train_size,\n",
    "    tiny_imagenet_val_size,\n",
    ")\n",
    "\n",
    "\n",
    "n_epochs = 2\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    generator=partial(tiny_imagenet_generator, split='train', shuffle=True),\n",
    "    output_signature=tiny_imagenet_signature,\n",
    ").repeat(n_epochs).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    generator=partial(tiny_imagenet_generator, split='val', shuffle=True),\n",
    "    output_signature=tiny_imagenet_signature,\n",
    ").repeat(n_epochs).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling training\n",
    "\n",
    "Now we come to the important part, the profiling. In this part we will have to include the checkpointing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling is done via callback\n",
    "profiling_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='logs/base-tf',\n",
    "    histogram_freq=1,\n",
    "    profile_batch=\"15,25\",\n",
    ")\n",
    "\n",
    "# Compile model as usual\n",
    "resnet18.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to add profiling callback\n",
    "resnet18.fit(\n",
    "    train_dataset,\n",
    "    epochs=n_epochs,\n",
    "    steps_per_epoch=(tiny_imagenet_train_size // batch_size),\n",
    "    callbacks=[profiling_callback],\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=(tiny_imagenet_val_size // batch_size),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "1. Look at the profiling results in tensorboard. To do this, follow the instructions in README.md\n",
    "2. Try to follow the Performance Recomendation and try again by modifying the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling is done via callback\n",
    "profiling_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='logs/improved-tf',\n",
    "    histogram_freq=1,\n",
    "    profile_batch=\"15,25\",\n",
    ")\n",
    "\n",
    "# Compile model as usual\n",
    "resnet18.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Remember to add profiling callback\n",
    "resnet18.fit(\n",
    "    train_dataset,\n",
    "    epochs=2,\n",
    "    steps_per_epoch=(tiny_imagenet_train_size // batch_size),\n",
    "    callbacks=[profiling_callback],\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=(tiny_imagenet_val_size // batch_size),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
